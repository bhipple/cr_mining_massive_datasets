Map Reduce
----------
Map(k, v) -> <k', v'>*
    One Map call for every (k,v) input pair
    Word count example:
        (k,v) is (document name, document contents)
        It then emits to <word, 1> for every word in the document
    5 word sequence:
        Same (k,v)
    Emits <5 word sequence, 1> from document
Group by key
    Sort the keys
Reduce(k', <v'>*) -> (k', <v''>*)
    One Reduce call per unique key k'
    All values v' with the same key k' are reduced together to form the new pair <k', v''>*
    Word count example:
        For each key k', count the number of v' values associated with it.


Distributed System Details:
---------------------------
Process Description
    We may have many map nodes. When map nodes emit their values, they decide which
    reduce node to send each particular <k', v'>* pair to by running it through
    a partitioning function (hashing).

    Input and final output are stored on the distributed file system (DFS)

    The map reducing framework tries to schedule map jobs for chunks on the same chunk server,
    to avoid costly data transfers.

    Whenever possible, intermediate results are stored on the local FS of map and reduce workers

    The output of a MapReduce task is often input to another MapReduce task

Rules of thumb for selecting M and R, the number of Map and Reduce tasks:
    Make M much larger than the number of nodes in the cluster
        This way each task is small, so if a node fails and we have (M-1) nodes available, one node won't have to do 2x the work!
    One DFS chunk per map

    R is usually smaller than M

Combiners and Partition Functions
---------------------------------
If the framework allows it and the programmer provides a combiner, then
    it can be used to aggregate some results in the mapper

combine(k, list(v1)) -> (k, v2)
    Usually, the combiner is the same function as the Reducer
    Combiner trick works ONLY IF reduce function is commutative and associative

partition(hashFunction, R)
    Instead of using the default hash function, we may want to do something else to ensure all the values we care about go to the same Reducer


Analysis of Large Graphs
------------------------
Page rank through stochastic adjacency matrices
Page    Power iteration: start with a guess for r, then keep multiplying r = r * M until the epsilon change in r is sufficiently small

This finds a stationary distribution for a random walk through the graph.
    That is, the probability of a surfer being at any given node at time t+1 is equal
    to the probability of a surfer being at any given node at time t

Random walks are called First Order Markov Processes
    For certain graphs, there is a unique stationary distribution and will eventually
    be reached for any initial probability distribution at time t = 0

On the page rank algorithm,
    Does r = Mr converge?
        The "Spider Trap" problem
            Characterized by a group in which all out-links are to other members of the group
            Nodes a and b just keep swapping their values back and forth indefinitely
            Eventually, a spider trap will absorb all importance in the group
        The "Dead End" problem
            Characterized by a page with no out-links
            A dead end b gets all of a's value, and then in the next iteration it has nowhere to pass it and the value gets lossed
            These cause importance "leaks"
    Random Teleports Solution for Spider Traps
        At each step, with probability B, will follow a random link
        With probability (1-B), will jump to some random page
        Commonly run with B in the range .8 to .9
    Random Teleports for Dead Ends
        If a node has out degree 0, it has a probability 1 to teleport to a random node

    Theory of Markov Chains:
        For any start vector, the power method applied to a Markov transition matrix P will converge to a unique positive stationary vector as long as P is stochastic, irreducible, and aperiodic.

        Stochastic Matrix
            Requires that each column sums to 1
            By adding random teleports to nodes with out degree 0, we create this propertyu

        Periodic
            A chain is periodic if there exists k > 1 such that the interval between two visits to some state s is always a multiple of k
            Because every node has a random teleport chance, we break deterministic periodic cycles

        Irreducible Matrix
            From any state, there is a non-zero probability of going from any one state to any other

    Implementation
        Input: Graph G and parameter Beta
        Assume the graph G has both spider traps and dead ends

        do:
            For all nodes, set them to the sum of weighted contributions from their in-links, times beta
            Then re-insert the leaked PageRank:
            For all nodes, r = r + (1 - S) / N, where S is the sum of all r on the column (whatever less than 1 was leaked by (1-B) or dead ends)
                This takes care of leak from dead ends and from not calculating the teleports explicitly
        while:
            The sum of differences in |r(t) - r(t-1)| < epsilon


Locality Sensitive Hashing
--------------------------
We break up the document into a set of k-grams, where each k-gram is a contiguous sequence of k letters
These are called shingles, and if k is relatively large (say 5 to 10), it will be the case that
similar documents share a relatively large number of shingles.  We may also hash these shingles into
4 bytes to compress them, and refer to them as tokens.

We then apply minhashing on the shingles|tokens.
