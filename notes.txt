// ============================================================================
//                                  Week 1
// ============================================================================
Map Reduce
----------
Map(k, v) -> <k', v'>*
    One Map call for every (k,v) input pair
    Word count example:
        (k,v) is (document name, document contents)
        It then emits to <word, 1> for every word in the document
    5 word sequence:
        Same (k,v)
    Emits <5 word sequence, 1> from document
Group by key
    Sort the keys
Reduce(k', <v'>*) -> (k', <v''>*)
    One Reduce call per unique key k'
    All values v' with the same key k' are reduced together to form the new pair <k', v''>*
    Word count example:
        For each key k', count the number of v' values associated with it.


Distributed System Details:
---------------------------
Process Description
    We may have many map nodes. When map nodes emit their values, they decide which
    reduce node to send each particular <k', v'>* pair to by running it through
    a partitioning function (hashing).

    Input and final output are stored on the distributed file system (DFS)

    The map reducing framework tries to schedule map jobs for chunks on the same chunk server,
    to avoid costly data transfers.

    Whenever possible, intermediate results are stored on the local FS of map and reduce workers

    The output of a MapReduce task is often input to another MapReduce task

Rules of thumb for selecting M and R, the number of Map and Reduce tasks:
    Make M much larger than the number of nodes in the cluster
        This way each task is small, so if a node fails and we have (M-1) nodes available, one node won't have to do 2x the work!
    One DFS chunk per map

    R is usually smaller than M

Combiners and Partition Functions
---------------------------------
If the framework allows it and the programmer provides a combiner, then
    it can be used to aggregate some results in the mapper

combine(k, list(v1)) -> (k, v2)
    Usually, the combiner is the same function as the Reducer
    Combiner trick works ONLY IF reduce function is commutative and associative

partition(hashFunction, R)
    Instead of using the default hash function, we may want to do something else to ensure all the values we care about go to the same Reducer


Analysis of Large Graphs
------------------------
Page rank through stochastic adjacency matrices
Page    Power iteration: start with a guess for r, then keep multiplying r = r * M until the epsilon change in r is sufficiently small

This finds a stationary distribution for a random walk through the graph.
    That is, the probability of a surfer being at any given node at time t+1 is equal
    to the probability of a surfer being at any given node at time t

Random walks are called First Order Markov Processes
    For certain graphs, there is a unique stationary distribution and will eventually
    be reached for any initial probability distribution at time t = 0

On the page rank algorithm,
    Does r = Mr converge?
        The "Spider Trap" problem
            Characterized by a group in which all out-links are to other members of the group
            Nodes a and b just keep swapping their values back and forth indefinitely
            Eventually, a spider trap will absorb all importance in the group
        The "Dead End" problem
            Characterized by a page with no out-links
            A dead end b gets all of a's value, and then in the next iteration it has nowhere to pass it and the value gets lossed
            These cause importance "leaks"
    Random Teleports Solution for Spider Traps
        At each step, with probability B, will follow a random link
        With probability (1-B), will jump to some random page
        Commonly run with B in the range .8 to .9
    Random Teleports for Dead Ends
        If a node has out degree 0, it has a probability 1 to teleport to a random node

    Theory of Markov Chains:
        For any start vector, the power method applied to a Markov transition matrix P will converge to a unique positive stationary vector as long as P is stochastic, irreducible, and aperiodic.

        Stochastic Matrix
            Requires that each column sums to 1
            By adding random teleports to nodes with out degree 0, we create this propertyu

        Periodic
            A chain is periodic if there exists k > 1 such that the interval between two visits to some state s is always a multiple of k
            Because every node has a random teleport chance, we break deterministic periodic cycles

        Irreducible Matrix
            From any state, there is a non-zero probability of going from any one state to any other

    Implementation
        Input: Graph G and parameter Beta
        Assume the graph G has both spider traps and dead ends

        do:
            For all nodes, set them to the sum of weighted contributions from their in-links, times beta
            Then re-insert the leaked PageRank:
            For all nodes, r = r + (1 - S) / N, where S is the sum of all r on the column (whatever less than 1 was leaked by (1-B) or dead ends)
                This takes care of leak from dead ends and from not calculating the teleports explicitly
        while:
            The sum of differences in |r(t) - r(t-1)| < epsilon


Shingles
--------
We break up the document into a set of k-grams, where each k-gram is a contiguous sequence of k letters
These are called shingles, and if k is relatively large (say 5 to 10), it will be the case that
similar documents share a relatively large number of shingles.  We may also hash these shingles into
4 bytes to compress them, and refer to them as tokens.

We then apply minhashing on the shingles|tokens.

Minhashing
----------
We will use the Jacard similarity function to measure set similarity: (size of intersection / size of union)
Convert the shingle bags into a boolean matrix:
    Each row is an element from the universal set of k-shingles
    Each column is a set, with a 1 at index (i, j) if set j has shingle i
    We want to find similar columns

    Let's say we have the rows randomly permuted
    Let the minhash function h(C) = the number of the first row of the randomly permuted ordering in which column C has 1
    We will create a signature for each column using a number of such independent minhash functions (say, 100)

    It turns out that the probability over all permutations of the rows that h(C1) == h(C2) is the same as
    JacardSim(C1, C2). Therefore, the similarity of signatures is the fraction of the minhash functions
    in which they agree.

    Process:
    1) Take the boolean input matrix
    2) For each one of a large number of random permutations,
    3) Calculate the Signature Matrix M, in which each row is the minhash of all columns on a given permutation,
        i.e., M(i, j) = the first row where column j had a 1 in permutation i
    4) To look at the similarity of two columns, we look at the number of elements they share in common

Doing actual permutations is too expensive, so we simulate it
We pick, say, 100 hash functions for each minhash that we want to simulate

For each column c and each hash function h_i, keep a "slot" M(i, c)
The intent is that M(i, c) will become the smallest value of h_i(r) for which column c has 1 in row r
That is, h_i(r) gives order of rows for the ith permutation

for each row r:
    for each hash function h_i:
        compute h_i(r)
    for each column c:
        if c has 1 in row r:
            for each hash function h_i:
                M(i, c) = min(M(i, c), h_i(r)

Locality Sensitive Hashing
--------------------------
Idea: we take a collection of all elements and want to generate a list of candidate pairs whose similarity
must be evaluated

For signature matrices, we hash columns to many buckets and make elements of the same bucket candidate pairs

First, we pick a simularity threshold t in (0, 1) that describes what fraction of rows need to be the same
Then, we will hash columns of the signature matrix M several times such that similar columns get similar hash bucket values

// ============================================================================
//                                  Week 3
// ============================================================================
Nearest Neighbor Machine Learning
---------------------------------
We need 4 things to make a nearest neighbor classifier:
    1) Distance metric?
    2) How many nearest neighbors k will we look at?
    3) (Optional) Function for weighting nearest k neighbors?
    4) How to fit with the k local points?  For example, if k == 1, then we just
        set our prediction to be equal to the nearest neighbor, but otherwise we
        may need to combine them in some way to make our classification

Using locality sensitive hashing
The goal: We have a set of points P, which are labeled (classified).
    When an unknown point q comes in, we want to:
    1) Find the k nearest neighbors to q, using our distance metric
        This can be done in near-constant time with locality sensitive hashing
    2) Apply our weighting function and analyze the points to classify q

The Market-Basket Model
    Given a large set of items, and a large set of baskets containing one or more items from the set of items,
    find items that are frequently in the same basket.

    A "support" for itemset I is the number of buckets containing all items in I

    An association rule is a rule saying that if a basket contains items i1, i2, ..., in, then it
    likely also contains item j with some confidence p

Approaches to organization of main memory for counting pairs
    Assume an integer takes 4 bytes
    Triangular Matrix Approach
        Two dimensional array where where A[i][j] only exists if i < j
        Requries 4 bytes per pair of items
        Number the items by consecutive integers starting at 1, with a separate hash table that translates from original name -> incremental integer name
        Cout all {i, j} such that i < j
        Keep pairs in the order {1,2}, {1,3}, ... , {1,n}, {2,3}, {2,4}, ... , {2,n}

        Thus what we have is really a 1-dimensional array, and we can use the function
            f(i, j) = (i - 1) * (n - i/2) + j - i
        to map from the matrix position to the pair in the array.

        There are n choose 2 pairs ~= n^2/2 pairs
        We use 4 bytes per pair, so we need 2n^2 bytes

    Keep a hash table of trios [i, j] -> c, where c is the current count of the items
        Requires 12 bytes per pair, but only if the pair exists

A priori:
    Monotonicity: If a set of items appears at least s times, then so does every subset of s

    This allows us to make successive passes, starting with a set of frequent tuples of size
    (k-1) and building the set of frequent tuples of size k

    A pass consists of:
        Input:
            C_k, the set of all candidate k-sets
        Output:
            L_k, the set of all frequent k-sets

        The C_k are constructed by taking each frequent k and appending it to every (k-1) set in which k is not present

    After each pass we may want to create a hash-table that re-numbers the sets sequentially, so that they take up less space when stored

Park-Chen-Yu (PCY) Algorithm
    Takes advantage of the fact that we have a lot of idle memory during the first pass of A-Priori

    We hash all pairs into buckets that do not contain the pairs themselves, just a count.
        For each basket, enumerate all its pairs, hash them, and increment the resulting bucket count by 1
    A bucket is frequent if its count is at least equal to the support threshold
        Afterwards, we convert this large hash table of buckets into a simple bitmap vector that has the
        key [i, j] to either a 0 or 1, to indictate whether the bucket is frequent.

    On pass 2, then, we ONLY count pairs that hash to frequent buckets!

    We count all pairs {i, j} that meet these conditions:
        1) Both i and j are frequent items themselves
        2) The pair {i, j} hashes to a bucket whose bit in the bit vector is 1

    Because the frequent buckets and pairs are scattered all over the place, we have to use the [i, j, c] hash table
    appraoch for the second pass. This means that we must eliminate 2/3 of the candidate pairs for PCY to beat
    A-Priori, since otherwise we could use the triangular matrix approach

Multistage Algorithm
    Further improvement on PCY
    Key idea: After pass 1 of PCY, we make another pass and rehash with a different hash function
        When making the second pass, we ONLY increment the bucket with the second hash function
        if {i, j} hashes to a frequent bucket in the first hash function

        This reduces false positives from hash collisions

    Count only those pairs {i, j} that satisfy these candidate pair conditions:
        1) Both i and j are frequent items
        2) Using the first hash function, the pair hashes to a bucket whose bit in the first bit-vector is 1
        2) Using the second hash function, the pair hashes to a bucket whose bit in the first bit-vector is 1

    Note here that the hash functions need to be independent of each other

Multihash Algorithm
    Key idea: use several independent hash tables on the first pass

    Each hash table will have to be smaller (fewer buckets), which means that there will be more collisions.
    However, we will have more tables, so it may be the case that we are able to eliminate far more
    false positives.

Alternatives that use <=2 passes
    Simple Algorithm
        Take a random sample of the market basts, and then just ran a-priori or any of its improvements
        in main memory, so you don't pay for disk I/O

        Scale back the support threshold accordingly

        Trades accuracy for a huge boost in speed

        Optionally, go back and make a second pass to using the entire dataset to eliminate false positives.
        This won't help us for false negatives, though.

    Toivonen's Algorithm
        Starts with the simple algorith, but lowers teh threshold slightly for the sample
        Calculate the negative border of sets, where a given set {A,B,C,D} is in the negative border iff:
            1) It is not frequent in the sample
            2) All of {ABC}, {BCD}, {ACD}, and {ABD} are frequent
            That is, it is all item sets that are not frequent but the every (k-1)-set in it IS frequent

            (Note: We define the empty set as frequent)

Bloom Filters
    Consists of:
    1) An array of n bits, initially all 0's
    2) A collection of hash functions h_1 to h_k, each of which
    maps key values to n buckets corresponding to the n values
    of the bit array
    3) A set S of m key values

    We then apply each hash function on each key m in S, and
    set the corresponding bit to 1.

    In the future, when a new candidate is evaluated, if it
    does not hash to a 1 with every single hash function, then
    it has NOT been seen before.

    If it does hash to 1, then it has either been seen before
    or is a false positive.

    Probability of a false positive (key not in S passing all hashes):

    Let m be the number of keys in S, n be the number of bits,
        k be the number of hash functions. Then:
        P(bit remains 0) = e^(-km/n)

    If the fraction of 0-bits is too low, then the Bloom filter
    will not be particularly useful as there will be a large number
    of false positives let through

    P(false positive) = (1 - e^(-km/n))^k

    That is, it's the probability of randomly getting a 1-bit on each
    of the k hash functions. Since a false positive doesn't hash to 1
    on by its own virtue for any hash function, it needs to get lucky
    every time.
WEEK 3
------
Community Interaction In Graphs

Affiliation Graph Model: A generative model B(V, C, M, {p_c}) for graphs:
    Nodes V, Communities C, Memberships M
    Each community c has a single probability p_c

    This model generates the links (edges) in the network graph:
        For each pair of nodes in community A, we connect them with probability p_a
        The overall edge probability is P(u,v) = 1 - product (1 - p_c) for all c in M_u intersect M_v,
        where M_u is the set of communities node u belongs to.

        If u,v share no communities, then P(u,v) = epsilon
        Intuitively: if at least one community says "YES" to the two candidate nodes u,v, then we create an edge
        Answers the question: "Given a pair of nodes and the set of communities to which they both belong,
        how likely are these two nodes to be connected"

    This model has the flexibility to express non-overlapping, overlapping, OR nested graph clusters

AGM -> BigCLAM
    As a generalization of AGM, let's model F_ua as the membership strength of node u to the community A,
    where a value of 0 is no membership; this gives more nuance over the binary membership, so:

    The probablity that two nodes u and v are connected given a commuunity A:
    P_a(u,v) = 1 - exp(-F_ua * F_va)

    We can then define a community membership strength matrix F
    Every row is a node
    Every column is a community
    Every F(v,a) gives the strength of the node v to the community a

    Generalizing the above P(u,v) equation to all communities c:
    P(u,v) = 1 - product_c(1 - P_c(u, v))

    Generalizing the dot product with vectorization:
    P(u,v) = 1 - exp(-F_u * F_v')

    Problem statement: Given a network G(V, E), estimate the matrix F

    We write a "likelihood" function that gives the likelihood that the graph
    G(V, E) was generated from the matrix F.  Our task then is to find a matrix
    F that maximizes the likelohood function

    [product over u,v in E] p(u,v) * [product over u,v not in E] (1 - p(u,v))

    In order to avoid rounding errors with multiplying many small numbers, we take the log
    of both sides to convert to addition!

    Find F that maximizes l(F):
        l(F) = 
    We want to find an F that maximizes the likelihood:


// ============================================================================
//                                  Week 4
// ============================================================================
Recommender Systems
There are three primary approaches:
    Content-based
    Collaborative Filtering
    Latent factor based

Content-Based Recommendations
    Recommend items to customer x similar to previous items rated highly by x

    TF-IDF: Term Frequency * Inverse Document Frequency
        TF_ij = f_ij / (max_k * f_kj)
        That is, the Term Frequncy of an item i in a document j, divided by the maximum frequency for that term i in any of the documents

        IDF_i = log(N / n_i), where N = the total number of documents, n_i is the nubmer of times i shows up in this document

    Given a user profile x, and an item profile i, we compute the cosine similarity between the x and i vectors
    The smaller the angle between these vectors, the greater the similarity.
    We recommend the item with the smallest angle to the user x

    Advantages:
        Don't need data from other users to make recommendations
        Able to recommend to users with unique tastes
        Able to recommend new and unpopular items (no "first-rater problem")
        We can provide explanations for why something was related by looking at content features

    Disadvantages:
        Finding the appropriate features is hard and/or labor intensive
        Overspecialization: never recommends an item outside the user's content profile
            Fails to capture multiple interests of users
        Cold-start problem for new users

Collaborative Filtering
    For a given user x, find set N of other users whose ratings are "similar" to x's ratings,
    then estimate x's ratings based on ratings of users in N

    Given users x and y with rating vectors r_x, r_y
    We compute the centered cosing by first normalizing the rows (subtract the mean of r_x from every rated entry),
    then fill in 0s in the empty slots (items not rated)

    This means that the sum of ratings in any particular row is 0; 0 is the average rating for every user

    Then we compute the similarity between users X and Y using the centered cosine similarity:
        sim(X, Y) = cos(r_x, r_y)
    (The centered cosine similarity is also known as the Pearson Correlation)

    We can do user-ser collaborative filtering, or we can do item-item collaborative filtering

    User-user:
        For a user x and an item i, we predict how x would rate i by looking at a neighborhood of k users most
        similar to x who have also rated i, and using their rating to generate a rating according to some formula

    Item-item:
        For an item i, we find similar items to i that x has rated,
        and estimate its rating based on x's ratings for the similar items

    In theory, these should be similar; but in practice, item-item filtering dramatically outperforms
    user-user filtering.
        It turns out that items are inherently "simpler" than users, and are easier to classify into
        a category. Users tend to be much more varied

        Therefore the notion of item similarity is much more meaningful than the notion of user similarity

Dimensionality Reduction
    Singular Value Decomposition

    Eigenvalues
        Let M be a square matrix, let lambda be a constant and e a nonzero column vector
        with the same number of rows as M. Then lambda is an eigenvalue of M
        and e is the corresponding eigenvector of M if (M * e = lambda * e)

        In order to avoid infinite eigenvectors scaled by constant factors for each lambda,
        we insist that the eigenvector be a unit vector whose first nonzero component is positive

        We can find the principal eigenvalue through power iteration:
        Givem M, random x, repeatedly set:
        x = M*x / norm(M * x)

        Once this converges, the principal eigenvalue is:
        lambda = x' * M * x

        Then we can subtract out the principal eigenpair from M to find the next eigenpair
        M = M - lambda * x * x'

    Principal Component Analysis
        Given a set of tuples, treat them as a matrix M and find the eigenvectors
        for MM' or M'M. In practice we choose between the two M representations based
        on which will be smaller, but it can be proven that they have equivalent eigenvalues
        and the extra eigenvalues of the larger representation are 0s.


// ============================================================================
//                                  Week 5
// ============================================================================
Clustering
    Agglomerative hierarchical clustering
        Centroid and clustroid for euclidean and non-euclidean spaces in O(N^2 * log(N)) runtime
    K-Means Clustering
    BFR
    CURE - Clustering Using REpresentatives

Bipartite Graph Matching
    Greedy Algorithm

    Performance-based Advertising and the AdWords Problem
        Given:
            As set of bids by advertisers for search queries
            A click-through rate for each advertiser-query pair
            A budget for each advertiser
            A limit on the number of ads to be displayed we each search query
        Respond to each search query with a set of advertisers such that:
            The size of the set is no larger than the limit on the number of ads per query
            Each advertiser has bid on the search query
            Each advertiser has enough budget left to pay for the ad if it is clicked upon

        BALANCE Algorithm:
            FOr each query, pick the advertiser with the largest unspent budget

// ============================================================================
//                                  week 6
// ============================================================================
Support Vector Machines for Classification
    For a d-dimensional dataset, there is a set of support vectors with cardinality
    (d+1) which uniquely defines the hyperplane that maximizes the classification margin

    SVMs with Stochastic Gradient Descent in an Online Learning example
        We can make a stochastic adjustment whenever a new training example comes in

Decision Trees
    We build a tree where each node is one of two types:
    1) Prediction node that predicts a value, or
    2) Computation node with a function that returns one of its children based on the value of the function on X

    We build the tree recursively by either splitting or classifying:
        At each split node, we measure how much a given attribute X tells us about the class Y
        We choose a split that maximizes this information gain:
            Find a split (X, v) that creates D, D_l, D_r (parent, left, right child datasets)
            which maximizes:
            |D| * Var(D) - (|D_l| * Var(D_l) + |D_r| + Var(D|r))

        Intuitively, this means that we try to minimize the variances of the child datasets

        We stop splitting when the parent node is "pure", which could mean either that its
        variance is below some parameter epsilon, or it could mean that the number of examples
        in the leaf is small (below some threshold cardinality C)

    In a prediction node, we either predict the most common node from the training set that landed
    in that node (most common), or we could build a small linear regression classifier that runs
    within that node (cool idea, but less commonly used)

    We define the entropy of a dataset X's distribution with:
        H(X) = - sum[from j = 1 to m](p_j * log(p_j))
            Where m is the total number of options values in X's domain, and
            p_j is the probability of the dataset having that value j

    High Entropy --> Data is very random
    Low Entropy --> X has interesting trends in its dataset

    We define the specific conditional entropy as the entropy of Y among only those
    records in whcih X has a value v, that is, H(Y | X=v)

    IG is the "information gain" function
        IG(Y|X) = H(Y) - H(Y|X)

        How much is entropy changed if we known X?
        Tells us how much information of Y is contained in X
        We want to pick features that have high IG

// ============================================================================
//                                  Week 7
// ============================================================================
Generalized Locality Sensitive Hashing
    The family of Minhash functions applied to the Jacard distance are one specific
    example of a locality sensitive function family

    We can create a family of functions that when applied to a space and a distance metric,
    1) Are more likely to make close pairs candidate pairs than distant pairs
    2) Are statistically indepdendent, so we can use the product rule for independent events
    3) Are efficient, in that they identify candidate pairs faster than O(n^2) and can be
       combined into functions which are better at avoiding false positives and false negatives

    Given a elements x and y and a function f, we will denote f(x) = f(y) to mean
    that these elements are considered candidates for comparison (potentially similar)

    Let d1 < d2 be two distances according to some distance measure d. A family F
    of functions is said to be (d1, d2, p1, p2)-sensitive if for every f in F:
    1) If d(x,y) < d1, then the probability that f(x)=f(y) is at least p1
    2) If d(x,y) > d2, then the probability that f(x)=f(y) is at most p2

    We can combine sets of functions f out of F to drive p1 higher and p2 lower through
    AND / OR combinations, but then for every element we have to compute its values with more functions

Random Hyperplanes with cosine distance

Efficient Computation of PageRank
    Since the transition matrix M is extremely sparse, we want to represent it by its
    non-zero edges instead of in its full NxN adjacency matrix form

    Source  Degree  Destinations
    A       3       B, C, D
    B       2       A, D
    C       1       A
    D       2       B, C

    Here we use 4 bytes for each row (degree) + 4 bytes per destination in that row

    In the PageRank formulation, we do many repetitions of:
    v2 = BMv + (1-B)e/n
    which is just a matrix vector multiplcation. With the size of the web today,
    it is likely that even the vector v is too big to fit in memory.
    We can break M into vertical stripes and v into corresponding horizontal stripes
    to send portions to MapReduce processes (Section 2.3.1)

Topic-Specific PageRank
    We do as before on the PageRank algorith, but for a given topic we
    create a set of websites relevant to the topic and ONLY do teleports
    to pages in that set

    Recall that the PageRank formulation was down through repeated iterations of:
    v = Beta * M * v + (1-beta) * e/n, where e is a vector of all ones and length n

    Now we will generate a topic set S, and set e such e[i] = 1 when i is in S, and 0 otherwise
    We replace the denominator n with the cardinality of S, |S|
    v = Beta*M * v + (1-beta) * e / |S|

SimRank
    Random walks from a fixed node on k-partite graphs
    That is, do a random walk starting from node u, where the teleport set S = {u}
    This isn't scalable on a web-scale since it needs to be run from every node, but it
    gives you scores for how connected any arbitrary node v is to u by looking
    at the PageRank value!

Hubs-and-Authorities
    HITS (Hypertext-Induced Topic Selection)
    Each page gets two scores, with links as votes:
        Hub - quality as an expect
            Sum of votes of authorities pointed to
        Authority - quality as content
            Sum of votes coming from expert
    We create a link matrix L, such that L(i,j) = 1 if there is a link from page i to page j, and 0 otherwise
    We use the vector h, such that h(i) denotes the hub score of the ith page
    Use vector a for the authority score of the ith page
    Then:
        h = lambda * L * a
        a = mu * L' * h

    Note that we use L' for a, since we want to reverse the links to get a sense of its value as an authority

    Algorithm involves repeating:
        1) Compute a = L' * h, and scale so the largest component is 1 (otherwise we infinitely increase in value)
        2) Compute h = L * a, and scale again

TrustRank
    Topic-specific PageRank with a teleport set of trusted web pages
    This avoids the problem of random teleports shifting importance into massive
    spam farm clusters
